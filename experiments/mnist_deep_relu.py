import numpy as np
import sys
import os

# Path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from engine.network import NeuralNetwork
from engine.layers import DenseLayer, ActivationLayer
from engine.activations import Activations
from engine.loss import Loss
from mnist_loader import load_mnist_data

# 1. à¶¯à¶­à·Šà¶­ 5,000à¶šà·Š à¶½à·à¶©à·Š à¶šà¶»à¶¸à·” (à¶¯à·à¶±à·Š à¶…à¶´à·š engine à¶‘à¶šà¶§ à¶½à·œà¶šà·” à¶¯à¶­à·Šà¶­ à¶¯à·’à¶»à·€à¶±à·Šà¶± à¶´à·”à·…à·”à·€à¶±à·Š!)

(x_train, y_train), (x_test, y_test), (y_train_raw, y_test_raw) = load_mnist_data(limit=5000)

net = NeuralNetwork()

# Layer 1: Input (784) -> Hidden 1 (128)
net.add(DenseLayer(784, 128))
net.add(ActivationLayer(Activations.relu, Activations.relu_derivative))

# Layer 2: Hidden 1 (128) -> Hidden 2 (64) - à¶…à¶½à·”à¶­à·Š Deep Layer à¶‘à¶š
net.add(DenseLayer(128, 64))
net.add(ActivationLayer(Activations.relu, Activations.relu_derivative))

# Layer 3: Hidden 2 (64) -> Output (10)
net.add(DenseLayer(64, 10))
net.add(ActivationLayer(Activations.softmax, Activations.softmax_derivative))

net.set_loss(Loss.cross_entropy, Loss.cross_entropy_derivative)

# 2. Training (Learning rate à¶‘à¶š 0.01 à·€à¶œà·š à¶´à·œà¶©à·’ à¶…à¶œà¶ºà¶šà·Š à¶­à¶¶à¶±à·Šà¶± ReLU à·€à¶½à¶¯à·“)
print("ðŸš€ Training Deep ReLU Network...")
net.train(x_train, y_train, epochs=100, learning_rate=0.0001)

# 3. Save the best model
net.save('mnist_deep_model.pkl')


