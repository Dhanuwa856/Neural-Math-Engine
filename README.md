# Neural-Math-Engine ğŸ§ ğŸ”¥
### Building Deep Learning from Scratch with Pure Mathematics

This project is a high-level implementation of a Neural Network engine built using only **Python** and **NumPy**. It aims to demonstrate the underlying mathematics of AI, including Linear Algebra, Calculus, and Gradient Descent, without relying on frameworks like TensorFlow or PyTorch.

## ğŸš€ The Achievement: Solving XOR
The engine successfully solved the classic **XOR problem**, which is a non-linearly separable logic gate. 
- **Training Accuracy**: ~99%
- **Final Loss (MSE)**: 0.002
- **Architecture**: 2 Input Neurons â†’ 4 Hidden Neurons (Sigmoid) â†’ 1 Output Neuron (Sigmoid)



## ğŸ§® Mathematical Foundations
This project proves the core mechanics of Deep Learning:
- **Forward Propagation**: Implementing $Z = W \cdot X + b$ using vectorized Dot Products.
- **Activation Functions**: Manual implementation of Sigmoid, Tanh, and ReLU with their respective derivatives.
- **Backpropagation**: Applying the **Chain Rule** to calculate gradients and update weights.
- **Loss Functions**: Mean Squared Error (MSE) calculation and its derivation for optimization.



## ğŸ“‚ Project Structure
```text
Neural-Math-Engine/
â”‚
â”œâ”€â”€ engine/             # The Core AI Engine
â”‚   â”œâ”€â”€ matrix.py       # Linear Algebra operations
â”‚   â”œâ”€â”€ activations.py  # Sigmoid, ReLU, Tanh
â”‚   â”œâ”€â”€ loss.py         # MSE & Derivatives
â”‚   â”œâ”€â”€ layers.py       # Dense & Activation Layers
â”‚   â””â”€â”€ network.py      # Training & Prediction Loop
â”‚
â”œâ”€â”€ experiments/        # Practical Applications
â”‚   â””â”€â”€ xor_problem.py  # First successful test
â”‚
â””â”€â”€ README.md
```
## ğŸ› ï¸ How to Run
- **Clone the repository.**
- **Ensure you have NumPy and Matplotlib installed**: 
``` pip install numpy matplotlib. ```
- **Run the XOR experiment**:
````
python experiments/xor_problem.py
````
